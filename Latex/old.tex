

\paragraph{A case study: the Bowles--Ahmed--Schuld benchmark.}

To address these issues, \textcite{bowles2024better} develop an open-source benchmarking suite based on PennyLane and conduct a large-scale empirical study. They systematically compare 12 influential QML models on 6 binary classification tasks, which, by varying problem parameters, generate 160 individual datasets grouped into 10 benchmarks.\footnote{Code and datasets are publicly available; see \textcite{bowles2024better} for links.} The models include various variational quantum neural networks, convolutional QNNs, and quantum kernel methods, alongside standard ``out-of-the-box'' classical baselines. The datasets are deliberately \emph{synthetic} or heavily controlled to probe specific aspects of learning performance:
\begin{itemize}
  \item some benchmarks vary the \emph{input dimension} to test how models scale with feature count, and
  \item others vary a \emph{difficulty parameter} that controls separability, nonlinearity, or noise, in order to study robustness as tasks become harder.
\end{itemize}
Examples include linearly separable data, bars-and-stripes patterns, downscaled MNIST, a hidden-manifold model, the ``two curves'' dataset, and a construction based on multiple hyperplanes and parity labels.\cite{bowles2024better}

Despite being ``small-scale'' in terms of qubit numbers (practically limited to roughly 10--20 qubits because of simulation costs and the need for hyperparameter search), the study already reveals several consistent patterns:

\begin{itemize}
  \item \textbf{Classical models typically outperform quantum ones.}  
  Across benchmarks, standard classical classifiers generally achieve higher accuracy than the quantum models when both are given comparable levels of tuning. This holds even though the quantum models are taken from highly-cited proposals and implemented carefully.
  
  \item \textbf{Quantum circuits without entanglement perform surprisingly well.}  
  For many architectures, removing entangling gates---so that each qubit is processed independently---does not significantly harm performance, and sometimes even improves it. In other words, models that are provably or plausibly classically simulable at scale can match or surpass their fully entangling counterparts. This raises questions about whether the observed performance is due to genuinely quantum effects or to more generic properties such as nonlinearity and overparameterisation.
  
  \item \textbf{Hybrid models do not show a clear quantum advantage.}  
  For architectures where quantum circuits appear as layers inside a larger classical pipeline (e.g., quantum feature maps feeding classical networks or quantum kernels inside SVM-like models), the ``quantum layer'' does not consistently provide benefits beyond what a classical layer with similar structure could do. In some cases it behaves more like a noisy bottleneck than a useful feature extractor.
  
  \item \textbf{Certain encodings perform poorly.}  
  Models based on amplitude encoding, which theoretically allow exponentially many features in a small number of qubits, perform badly on the considered classification tasks, even if multiple copies of the input state are provided. This suggests that expressivity alone is not enough: the induced decision boundaries and inductive biases may be poorly matched to the structure of typical learning problems.
  
  \item \textbf{Failure on simple benchmarks.}  
  Perhaps most strikingly, several quantum models struggle on the simplest, linearly separable datasets in the benchmark. While linear classifiers and basic classical models can solve these tasks almost perfectly, many QML models fail to reach comparable performance, indicating optimisation difficulties or poorly aligned inductive biases.
\end{itemize}

Beyond headline numbers, these results raise deeper conceptual questions: What inductive biases do current quantum models implement? In which regimes does ``quantumness'' (for example, entanglement or non-classical kernels) actually help? And which specific classes of problems might genuinely benefit from quantum properties rather than being solvable just as well by classical surrogates? \textcite{bowles2024better} argue that we currently lack satisfactory answers, precisely because our benchmarks and models have not been analysed along these dimensions.

\paragraph{Limitations of common datasets and the MNIST example.}

A concrete illustration of the benchmarking problem arises from the widespread use of MNIST as a ``standard'' image classification dataset, both in classical deep learning and in QML. For quantum circuits, however, running on error-free hardware is not yet possible, and full-state simulation with hundreds of qubits is infeasible. As a consequence, QML studies typically apply heavy dimensionality reduction (e.g., via PCA) to compress the original $784$-dimensional images into tens of features or less.\cite{bowles2024better}

While this makes classical simulation feasible, it also breaks comparability with the classical deep learning literature:
\begin{itemize}
  \item The baseline to beat becomes unclear: reducing MNIST to, say, two principal components turns, for example, the ``3 vs 5'' problem into two overlapping blobs where even simple models perform differently than on the original high-dimensional data.
  \item Many QML papers use hybrid pipelines where convolutional neural networks or tensor-network layers already extract features before the quantum circuit, so that the ``quantum'' part receives a heavily processed, low-dimensional representation. Any observed improvement then becomes difficult to attribute specifically to the quantum component.
\end{itemize}
\textcite{bowles2024better} therefore argue that carefully designed, lower-dimensional benchmarks---such as the 1D-MNIST variant or synthetic datasets with controlled structure---may be more appropriate for current QML studies than aggressively downscaled versions of high-dimensional benchmarks originally designed for deep learning.

\paragraph{Small-scale hardware and simulation constraints.}

Another layer of difficulty comes from the resource demands of QML benchmarks. Variational models, kernel methods, and hybrid architectures all rely on repeatedly evaluating quantum circuits over many training steps and hyperparameter configurations. In classical simulations, the cost grows exponentially with the number of qubits, and realistic hyperparameter searches (with cross-validation) require evaluating hundreds of thousands or millions of circuits.\cite{bowles2024better} Even with substantial CPU or GPU resources, this makes exhaustive benchmarks beyond $\sim 10$--$20$ qubits practically infeasible. On real devices, noise and limited shot budgets introduce yet another set of trade-offs.

As a result, many QML benchmarks operate in a regime that is simultaneously:
\begin{itemize}
  \item small enough to be classically simulable, so that classical baselines can in principle be pushed very far, and
  \item too small and noisy to display clear-cut asymptotic advantages that might only emerge at larger scales.
\end{itemize}
This tension makes it even more important to design benchmarks that do not over-interpret small performance differences or rely on cherry-picked scenarios.


\subsection{Variational Quantum Classifiers (VQCs)}
Variational Quantum Classifiers are quantum machine learning models based on \textit{variational quantum algorithms (VQAs)}. These algorithms combine a quantum circuit that depends on a vector of parameters with a classical optimization routine to minimize a cost function.

A VQC receives a classical input vector $\mathbf{x} \in \mathbb{R}^n$. First, the data is encoded into a quantum state using a \textit{quantum feature map}, a unitary transformation $U_\phi(\mathbf{x})$ that acts on the initial state:
\[
\ket{\psi_\mathbf{x}} = U_\phi(\mathbf{x}) \ket{0}^{\otimes n}
\]
Here, rotation gates are used to encode classical features into quantum states. For each feature \( x_j \), a corresponding qubit \( j \) is rotated by an angle proportional to the feature:
\[
\theta_j = c \cdot x_j
\]
where \( c \) is a scaling constant. Typically, a rotation gate such as \( R_y(\theta_j) \) or \( R_z(\theta_j) \) is applied. This operation rotates the qubit state on the Bloch sphere, embedding the data into a quantum Hilbert space

Next, a \textit{parameterized quantum circuit} $U(\boldsymbol{\theta})$ is applied to the encoded state. This circuit consists of quantum gates, again typically rotation gates, with trainable parameters $\boldsymbol{\theta}$ independent of the input data. These are analogous to weights in NNs.:
\[
|\psi_{\text{out}}\rangle = U(\boldsymbol{\theta}) |\psi_x\rangle
\]

After processing, the output state is \textit{measured}. The measurement yields after many runs or shots a probability distribution over classical bitstrings, and this outcome is interpreted to make a prediction. For binary classification, a common approach is to compute the expectation value of a Pauli-Z operator on a target qubit:
\[
f(\mathbf{x}) = \bra{\psi_{\text{out}}} Z \ket{\psi_{\text{out}}}
\]
The result, which lies between $-1$ and $1$, is post-processed into a binary decision.

The parameters $\boldsymbol{\theta}$ are updated using a classical optimizer that minimizes a cost function, typically comparing predicted outcomes to true labels. This hybrid loop—quantum state preparation, quantum transformation, classical evaluation, and classical optimization—is repeated until convergence \parencite{schuld2018circuit}. VQCs are attractive for implementation on near-term quantum devices because they can operate with shallow circuits and limited quantum resources. 




Among the most commonly employed quantum neural network (QNN) architectures in the literature are the dressed QNN and the re-uploading QNN. Both serve as representative baseline models for benchmarking due to their balance between conceptual simplicity and expressive power. 

The\textbf{ dressed variational quantum circuit or d-QNN} is embedded between two trainable classical linear layers. The first linear layer projects the input data into a vector of dimension equal to the number of qubits. These values are then encoded using angle encoding, which maps real-valued inputs to rotation parameters of single-qubit gates. After the trainable circuit ansatz and a local single qubit Pauli-$Z$ expectation value measurements, a second linear layer then maps the measurement results to the target output dimension. This architecture combines classical preprocessing and postprocessing with a quantum processing block, which increases flexibility but remains limited in expressivity since each data point is encoded only once \parencite{fellner2025quantum}.  

The \textbf{re-uploading variational quantum circuit }works by encoding classical data repeatedly at multiple layers of the circuit. Specifically, input features are embedded sequentially using exponential angle encoding, and trainable ansatz layers composed of parameterized gates are inserted between encoding steps. This repeated re-uploading enriches the expressive power of the model. As with the d-QNN, local Pauli-$Z$ expectation values are measured at the end of the circuit and transformed into the target dimension by a linear output layer \parencite{fellner2025quantum}.  


In terms of learning capacity, the d-QNN is structurally similar to a shallow neural network: classical preprocessing and postprocessing layers surround a single quantum circuit with fixed encoding, which limits the model essentially to linear decision boundaries. Its performance is therefore expected to be comparable to that of single-layer perceptrons. By contrast, the ru-QNN repeatedly re-encodes the data at multiple circuit depths, thereby enabling the representation of nonlinear functions. This increased expressivity makes the ru-QNN analogous to a multilayer perceptron (MLP), and its performance can be meaningfully compared to small neural networks.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{vqc_diagram.png} % Replace with actual filename
    \caption{
        Schematic of a Variational Quantum Classifier (VQC), adapted from \textcite{mittal2023variational}. The computation begins with $n$ qubits initialized in the state $\ket{0}^{\otimes n}$. The classical input vector $\mathbf{x} \in \mathbb{R}^n$ is embedded into a quantum state via a data-encoding unitary $U_\phi(\mathbf{x})$, known as the quantum feature map. This maps classical data into a high-dimensional Hilbert space. The state is then processed by a parameterized quantum circuit $U(\boldsymbol{\theta})$ with trainable parameters. These variational blocks are often repeated multiple times (denoted by hyperparameter depth $d$) to increase expressivity. After the final unitary transformation, the qubits are measured. The measurement outcomes are used to compute a cost function, typically defined over expectation values of observables. A classical optimizer then updates the parameters $\boldsymbol{\theta}$ to minimize the cost and this hybrid quantum–classical loop continues until it find this minimal cost convergence.}
    \label{fig:vqc_architecture_detailed}
\end{figure}




\section{Classical Machine Learning Models}

\subsection{Overview of Supervised Learning}
Supervised learning is a core approach in machine learning in which the goal is to infer a mapping from inputs to outputs using a dataset of labeled examples. Each example is represented as a pair $(\mathbf{x}, y)$, where $\mathbf{x} \in \mathbb{R}^n$ is an input vector composed of $n$ features, and $y$ is the corresponding target value. The learning algorithm or mathematical function, the model, attempts to approximate a function $f: \mathbb{R}^n \rightarrow \mathcal{Y}$ that best predicts $y$ given $\mathbf{x}$, where \(\mathcal{Y}\) denotes the set of all possible target values, also called the output or label space \parencite{murphy2012}.


Some models in supervised learning are called \textit{parametric}, meaning they assume a fixed form for the function $f$ and learn a set of parameters, typically denoted by a weight vector \(\mathbf{w} \in \mathbb{R}^n\)  and a bias term \(b \in \mathbb{R}\) during training. Others are \textit{non-parametric}, which means their structure or complexity grows with the amount of data, without assuming a fixed number of parameters \parencite{murphy2012}.


In classification tasks, models often produce an intermediate output $z = \mathbf{w}^\top \mathbf{x} + b$, called the score. This  is a real-valued scalar that serves as a bridge between raw input features through a decision or \textit{activation function} to final output labels \( \hat{y} \in \mathcal{Y} \). For example, a \textit{sign function} \( \hat{y} = \text{sign}(z)\) returns $+1$ or $-1$ based on the sign of $z$, while probabilistic models may use sigmoid or softmax functions to produce class probabilities. The geometry and dimensionality of the feature space (i.e., how many and which features are used to describe each $\mathbf{x}$) deeply influence how models learn and generalize.

\subsection{Linear Classifiers}
Linear classifiers are a class of models that separate data points in a feature space using a hyperplane defined by a weight vector $\mathbf{w} \in \mathbb{R}^n$ and a scalar bias term $b \in \mathbb{R}$. The model computes a linear combination $z = \mathbf{w}^\top \mathbf{x} + b$, and the sign of $z$ determines the predicted class. This leads to a decision function of the form $f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b)$, where $\text{sign}(\cdot)$ returns $+1$ for positive inputs and $-1$ for negative ones \parencite{rosenblatt1958}.

The vector $\mathbf{w}$ encodes the relative importance of each feature in the input $\mathbf{x}$, while the bias $b$ shifts the decision boundary. The decision boundary itself is the set of points $\mathbf{x}$ for which $\mathbf{w}^\top \mathbf{x} + b = 0$. Linear classifiers rely on the assumption that classes can be separated by such a boundary in the input space \parencite{rosenblatt1958}. They form the basis of many more complex models and are computationally simple, making them a foundational concept in machine learning.

\subsection{Logistic Regression}
Logistic regression extends the linear classifier framework by interpreting the output of the linear combination $z = \mathbf{w}^\top \mathbf{x} + b$ probabilistically. Specifically, it applies the \textit{sigmoid function}, $\sigma(z) = \frac{1}{1 + e^{-z}}$, to map real-valued inputs to the interval $[0, 1]$. This output is interpreted as the estimated probability that the input belongs to the positive class.

Given this probabilistic interpretation, the model is typically trained using \textit{maximum likelihood estimation}, where the parameters $\mathbf{w}$ and $b$ are chosen to maximize the likelihood of the observed labels under the model's predictions. The decision boundary is still linear in the input space, occurring at the point where the model assigns equal probability to both classes (i.e., where $z = 0$).

\subsection{Support Vector Machine (SVM)}
While linear classifiers separate the classes by finding any line, which makes them sensitive to outliers, Support Vector Machines (SVMs), by construction, are margin-based classifiers that aim to find the hyperplane separating two classes, and not focus on outlier points. This makes them more outlier-robust. Given training examples labeled $y_i \in \{-1, +1\}$, the SVM solves an optimization problem that finds the hyperplane $\mathbf{w}^\top \mathbf{x} + b = 0$ such that the distance between this hyperplane and the closest data points (called \textit{support vectors}) is maximized.

Maximizing the margin improves the model’s robustness to variations in the data. The supporting hyperplanes, those defining the margin boundaries, are given by \( w^T x + b = \pm 1 \). The distance from the decision boundary to each of these hyperplanes is \( \frac{1}{\|w\|} \). Therefore, the total margin width, which is the distance between the two margin hyperplanes, is \( \frac{2}{\|w\|} \). Maximizing it is equivalent to minimizing $\|\mathbf{w}\|^2$, which leads to the primal optimization formulation. This choice regularizes the model by penalizing large weights, which in turn reduces sensitivity to small fluctuations in input values.

The model predicts labels using the \textit{sign function}: $f(\mathbf{x}) = \text{sign}(\mathbf{w}^\top \mathbf{x} + b)$. In this setting, the sign of the inner product determines on which side of the hyperplane the input lies. If the data are not linearly separable, SVMs can be extended using kernel functions, which allow the model to behave as if the data were mapped into a higher-dimensional space, where a linear separator may exist, without explicitly computing that mapping.


\subsection{k-Nearest Neighbors (k-NN)}
The k-Nearest Neighbors (k-NN) algorithm is a non-parametric model that performs classification or regression based on the local structure of the data. Unlike parametric models, k-NN does not learn an explicit function during training. Instead, it retains all training examples and uses them directly during inference. When a new input $\mathbf{x}$ is presented, the algorithm computes the distances between $\mathbf{x}$ and all points in the training set—commonly using Euclidean distance—and selects the $k$ points that are closest.

For classification, the algorithm assigns the label that is most common among these $k$ neighbors. For regression, it typically outputs the average of their values. The underlying assumption in k-NN is that nearby points in the input space tend to have similar outputs. This notion of \textit{locality} is central to its functioning. However, it also means that the performance of k-NN depends heavily on the geometry of the feature space and on how distances are measured. Since it stores and compares all training data at prediction time, it is sometimes called a \textit{lazy learner}.

\subsection{Kernel Methods }
Kernel methods are a set of techniques that allow linear models to learn non-linear patterns by computing similarities between data points using a \textit{kernel function}. A kernel is a function $K(\mathbf{x}, \mathbf{x}')$ that measures how similar two inputs are, without explicitly mapping them into a higher-dimensional space. Instead, it leverages the mathematical property that inner products in this high-dimensional space can be computed directly via the kernel, a concept known as the \textit{kernel trick}.

One widely used kernel is the Gaussian kernel:
\[
K(\mathbf{x}, \mathbf{x}') = \exp\left(-\frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2\sigma^2}\right)
\]
This function assigns high similarity to points that are close in Euclidean space and low similarity to distant points. This similarity measure enables the model to construct complex, non-linear decision boundaries in the input space, while still solving a linear problem in an implicit feature space.

\subsection{Naive Bayes}
Naive Bayes is a family of probabilistic classifiers based on Bayes' theorem. It models the posterior probability of a class \( y \) given a feature vector \( \mathbf{x} = (x_1, x_2, \dots, x_n) \) by assuming \textit{conditional independence} between the features given the class. That is,
\[
P(y \mid \mathbf{x}) \propto P(y) \prod_{i=1}^n P(x_i \mid y).
\]
This strong independence assumption simplifies computation, making the model both tractable and interpretable, even though it may not hold exactly in practice.

Naive Bayes classifiers differ based on how $P(x_i \mid y)$ is modeled. For example, the \textit{Gaussian Naive Bayes} model assumes that continuous features are normally distributed within each class. The model computes the posterior probabilities for each class and assigns the one with the highest value. Despite its simplicity, Naive Bayes provides a probabilistic foundation for predicting an output class label and is especially well suited for problems involving discrete or categorical data.

\subsection{Gaussian Processes (GPs)}
Gaussian Processes (GPs) are a non-parametric, Bayesian approach to modeling functions. Instead of assuming a fixed form for the function $f(\mathbf{x})$, a GP defines a \textit{distribution over functions}, such that any finite set of inputs produces a multivariate Gaussian distribution over the outputs. This distribution is specified by a \textit{mean function} $m(\mathbf{x})$, which often defaults to zero, and a \textit{covariance function} $k(\mathbf{x}, \mathbf{x}')$, also known as a kernel, which encodes the similarity between inputs.

When used for classification, the GP provides a prior over latent function values, which are passed through a squashing function (such as the sigmoid) to obtain class probabilities. Because exact inference in this setting is intractable, approximations such as Laplace approximation or variational inference are used.

The key strength of Gaussian Processes is their ability to provide not only predictions but also \textit{uncertainty estimates}. That is, for each prediction, the model returns a distribution (typically Gaussian) reflecting its confidence. This is particularly useful when it is important to know how reliable a model’s prediction is, especially in domains like scientific modeling or risk-sensitive decision-making.

\subsection{Neural Networks (MLPs)}
Multilayer Perceptrons (MLPs) are a class of feedforward neural networks consisting of multiple layers of \textit{artificial neurons}. Each neuron computes a weighted sum of its inputs, applies a non-linear \textit{activation function} such as the rectified linear unit (ReLU), sigmoid, or hyperbolic tangent (tanh), and passes the result to the next layer. An MLP typically consists of an \textit{input layer}, one or more \textit{hidden layers}, and an \textit{output layer}.

Formally, each hidden layer performs a transformation of the form:
\[
\mathbf{h}^{(l)} = \sigma(\mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
\]
where $\mathbf{W}^{(l)}$ and $\mathbf{b}^{(l)}$ are the weights and biases of layer $l$, and $\sigma(\cdot)$ is the activation function. The model is trained using \textit{backpropagation}, which computes gradients of a loss function with respect to each parameter using the chain rule, followed by \textit{gradient descent} to update the weights.

Small MLPs, typically with one or two hidden layers and a moderate number of units, are capable of learning complex, non-linear relationships while maintaining tractability. Unlike kernel methods or instance-based models, MLPs \textit{learn internal representations} of the data during training, which makes them a flexible and general-purpose modeling approach.


\chapter{Methodology}
\label{chap:methodology}
This study investigates whether Variational Quantum Classifiers (VQCs) can achieve performance comparable to established classical supervised learning models on benchmark datasets. The comparison is meaningful because the selected baselines represent the main paradigms of machine learning: linear (Logistic Regression), kernel-based (Support Vector Machines, Kernel Ridge Regression), probabilistic (Naive Bayes, Gaussian Processes), distance-based (k-Nearest Neighbors), and neural (Multilayer Perceptron). Together, these models provide a comprehensive reference frame for situating VQCs within the landscape of supervised learning.



Past performances and expected. asdasds

\section{Experimental Setup}
\label{sec:setup}

\subsection{Datasets}
The experiment are to be conducted on benchmark datasets of varying complexity:
\begin{itemize}
    \item \textbf{Iris, Wine} – Small, well-known datasets with varying class balance and feature correlation.
    \item \textbf{Digits, Fashion-MNIST } – Higher-dimensional datasets to assess model scalability.
    \item \textbf{Blobs, Moons} – 2D separable and non-separable data for visual boundary analysis.
\end{itemize}

\subsection{Preprocessing}

All datasets were preprocessed to ensure comparability across models and to improve training stability. A key step in this process was \textit{normalization}, which refers to the transformation of input features so that they lie within a standardized range or distribution. This prevents features with larger numeric ranges from dominating the learning process and helps models converge more efficiently.

Two common normalization methods were used depending on the dataset and model:

\textbf{Min-Max Scaling} rescales each feature to a fixed interval, typically $[0,1]$, using the formula:
\[
x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
\]


\textbf{Standard Scaling (Z-score Normalization)} transforms each feature to have zero mean and unit variance:
\[
x' = \frac{x - \mu}{\sigma}
\]
where $\mu$ and $\sigma$ are the mean and standard deviation of the feature, respectively. 

\subsection{Dataset Split}
After normalization, each dataset was split into \textit{training} and \textit{test} sets to evaluate generalization performance. The training set is used to fit the model parameters, while the test set provides an unbiased estimate of how the model performs on unseen data. To ensure reproducibility, the split was performed using a \textit{fixed random seed}, which guarantees that the same data partitioning is obtained in every run. This makes experimental comparisons reliable and consistent.


\subsection{Classical Models}
Key hyperparameters of each.

\subsection{Quantum Model: Variational Quantum Classifier (VQC)}


\begin{itemize}
    \item VQC architectures considered.
    \item Feature maps: ZZFeatureMap, exponential angle encoding.

    \item Ansatz types: RealAmplitudes, EfficientSU2, TwoLocal.
    \item Variants: standard VQC, deep VQC, data re-uploading, dressed QNN.
    \item Qubit counts (after PCA).
    \item Circuit depths choices.
    \item Measurement scheme (Pauli-Z expectation values).
    \item Why these: compatible with Qiskit, reflects common practices in VQC research
\end{itemize}



\subsection{Training Procedure}


\begin{itemize}
    \item Classical models: scikit-learn training routines, CV setup.
    \item Quantum models: optimizers (SPSA, COBYLA, L-BFGS-B), shot counts, number of iterations.


    \item Backends: Aer simulator (ideal + noisy).
Aer simulator is the main simulation framework for running quantum circuits on a classical computer. It has different backends (execution modes),

Justification: chosen optimizers handle noisy gradients; simulators enable fair and reproducible testing.

\end{itemize}



\subsection{Evaluation Metrics}
The models were evaluated using:
\begin{itemize}
    \item Accuracy and F1-Score
    \item Training time and number of parameters
    \item Robustness to feature noise
    \item Performance under small data regimes
    \item Visualization of decision boundaries 
\end{itemize}

also why these

\subsection{Experimental Protocol and Tools and Environment}
\begin{itemize}
    \item Train/test split or k-fold cross-validation.
    
In addition, \textit{cross-validation} was optionally used during model selection and hyperparameter tuning to improve robustness. In $k$-fold cross-validation, the training set is divided into $k$ equally sized folds. The model is trained $k$ times, each time using $k-1$ folds for training and the remaining fold for validation. The results are then averaged across folds to obtain a more stable performance estimate. This approach reduces the risk of overfitting to a particular train-test split and provides a better understanding of model performance on limited data.

    \item Number of repetitions.
    \item Random seed control.
    \item Software: Python, scikit-learn, Qiskit.
    \item Hardware: CPU/GPU specs.
    \item For transparency, reproducibility, reliability.
\end{itemize}


\chapter{Results and Discussion}
\label{chap:results}

This chapter presents the experimental evaluation of Variational Quantum Classifiers (VQCs) in comparison to a range of classical supervised learning models. The discussion is divided into two main parts: a quantitative analysis of model performance and a qualitative discussion of interpretability, complexity, and applicability. The experiments were designed to assess classification accuracy, robustness, and computational feasibility under various settings.


\section{Quantitative Comparison}
\label{sec:quantitative}

This section presents numerical results comparing VQCs and classical models.

\begin{itemize}
    \item \textbf{Accuracy Across Datasets:} Tabulated and graphical comparison of test accuracy.
    \item \textbf{Robustness to Noise and Small Sample Sizes:} Controlled experiments with added noise or reduced training size.
    \item \textbf{Training Time and Resource Use:} Timing results and resource overhead.
    \item \textbf{Visualization of Decision Boundaries}
\end{itemize}

\section{Qualitative Analysis}
\label{sec:qualitative}

This section provides a broader discussion on the interpretability, feasibility, and practicality of the compared models.

\begin{itemize}
    \item \textbf{Model Strengths and Weaknesses:} Comparative review of performance patterns per model.
    \item \textbf{Interpretability and Complexity:} Transparency of decision functions, number of parameters, ease of tuning.
    \item \textbf{Generalization Capability:} Overfitting tendencies and consistency across datasets.
    \item \textbf{Feasibility in Real-World Scenarios:} Scalability and hardware compatibility for VQC; real-time application potential.
\end{itemize}



\chapter{Conclusion and Future Work}
% ... contenido ...

% References